#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢é‡å‘é‡å­˜å‚¨ç®¡ç†å™¨
æ”¯æŒå‘é‡å­˜å‚¨çš„å¢é‡æ›´æ–°ã€å¤ç”¨å’Œç‰ˆæœ¬ç®¡ç†
"""

import os
import json
import hashlib
import shutil
from datetime import datetime
from typing import List, Dict, Optional
from document_loader import load_documents, split_documents
from vector_store import create_vector_store, load_vector_store, create_embeddings
from langchain_community.vectorstores import FAISS


class IncrementalVectorStore:
    """å¢é‡å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, vector_store_path: str = "vector_store"):
        self.vector_store_path = vector_store_path
        self.metadata_file = os.path.join(vector_store_path, "metadata.json")
        self.vector_store = None
        self.metadata = {}
        
    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            print(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return ""
    
    def _get_documents_info(self, documents_dir: str) -> Dict:
        """è·å–æ–‡æ¡£ç›®å½•çš„ä¿¡æ¯"""
        docs_info = {}
        
        if not os.path.exists(documents_dir):
            return docs_info
        
        for filename in os.listdir(documents_dir):
            file_path = os.path.join(documents_dir, filename)
            if os.path.isfile(file_path) and filename.endswith(('.xlsx', '.xls', '.pdf', '.docx', '.txt')):
                file_hash = self._calculate_file_hash(file_path)
                file_stat = os.stat(file_path)
                
                docs_info[filename] = {
                    'hash': file_hash,
                    'size': file_stat.st_size,
                    'modified_time': file_stat.st_mtime,
                    'path': file_path
                }
        
        return docs_info
    
    def _load_metadata(self) -> Dict:
        """åŠ è½½å…ƒæ•°æ®"""
        if os.path.exists(self.metadata_file):
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
        
        return {
            'created_time': datetime.now().isoformat(),
            'last_updated': datetime.now().isoformat(),
            'documents': {},
            'total_documents': 0,
            'total_chunks': 0,
            'version': '1.0'
        }
    
    def _save_metadata(self):
        """ä¿å­˜å…ƒæ•°æ®"""
        os.makedirs(os.path.dirname(self.metadata_file), exist_ok=True)
        
        self.metadata['last_updated'] = datetime.now().isoformat()
        
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def check_updates_needed(self, documents_dir: str) -> Dict:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°"""
        current_docs = self._get_documents_info(documents_dir)
        self.metadata = self._load_metadata()
        stored_docs = self.metadata.get('documents', {})
        
        result = {
            'needs_update': False,
            'new_files': [],
            'modified_files': [],
            'deleted_files': [],
            'unchanged_files': []
        }
        
        # æ£€æŸ¥æ–°å¢å’Œä¿®æ”¹çš„æ–‡ä»¶
        for filename, info in current_docs.items():
            if filename not in stored_docs:
                result['new_files'].append(filename)
                result['needs_update'] = True
            elif stored_docs[filename]['hash'] != info['hash']:
                result['modified_files'].append(filename)
                result['needs_update'] = True
            else:
                result['unchanged_files'].append(filename)
        
        # æ£€æŸ¥åˆ é™¤çš„æ–‡ä»¶
        for filename in stored_docs:
            if filename not in current_docs:
                result['deleted_files'].append(filename)
                result['needs_update'] = True
        
        return result
    
    def create_or_update_vector_store(self, documents_dir: str, force_rebuild: bool = False) -> bool:
        """åˆ›å»ºæˆ–æ›´æ–°å‘é‡å­˜å‚¨"""
        print("ğŸ” æ£€æŸ¥å‘é‡å­˜å‚¨æ›´æ–°éœ€æ±‚...")
        
        # æ£€æŸ¥æ›´æ–°éœ€æ±‚
        update_info = self.check_updates_needed(documents_dir)
        
        if not update_info['needs_update'] and not force_rebuild and os.path.exists(self.vector_store_path):
            print("âœ… å‘é‡å­˜å‚¨å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°")
            self.vector_store = load_vector_store(self.vector_store_path)
            return True
        
        # æ˜¾ç¤ºæ›´æ–°ä¿¡æ¯
        if update_info['new_files']:
            print(f"ğŸ“„ æ–°å¢æ–‡ä»¶: {len(update_info['new_files'])} ä¸ª")
            for file in update_info['new_files'][:3]:
                print(f"  + {file}")
            if len(update_info['new_files']) > 3:
                print(f"  ... è¿˜æœ‰ {len(update_info['new_files']) - 3} ä¸ª")
        
        if update_info['modified_files']:
            print(f"ğŸ“ ä¿®æ”¹æ–‡ä»¶: {len(update_info['modified_files'])} ä¸ª")
            for file in update_info['modified_files'][:3]:
                print(f"  ~ {file}")
            if len(update_info['modified_files']) > 3:
                print(f"  ... è¿˜æœ‰ {len(update_info['modified_files']) - 3} ä¸ª")
        
        if update_info['deleted_files']:
            print(f"ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶: {len(update_info['deleted_files'])} ä¸ª")
            for file in update_info['deleted_files'][:3]:
                print(f"  - {file}")
        
        if update_info['unchanged_files']:
            print(f"âœ… æœªå˜æ›´æ–‡ä»¶: {len(update_info['unchanged_files'])} ä¸ª")
        
        # å†³å®šæ›´æ–°ç­–ç•¥
        if force_rebuild or len(update_info['deleted_files']) > 0 or len(update_info['modified_files']) > 0:
            # å®Œå…¨é‡å»º
            return self._full_rebuild(documents_dir)
        else:
            # å¢é‡æ›´æ–°
            return self._incremental_update(documents_dir, update_info['new_files'])
    
    def _full_rebuild(self, documents_dir: str) -> bool:
        """å®Œå…¨é‡å»ºå‘é‡å­˜å‚¨"""
        print("ğŸ”„ æ‰§è¡Œå®Œå…¨é‡å»º...")
        
        try:
            # åˆ é™¤æ—§çš„å‘é‡å­˜å‚¨
            if os.path.exists(self.vector_store_path):
                shutil.rmtree(self.vector_store_path)
            
            # åŠ è½½æ‰€æœ‰æ–‡æ¡£
            print("ğŸ“š åŠ è½½æ‰€æœ‰æ–‡æ¡£...")
            documents = load_documents(documents_dir)
            
            if not documents:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
                return False
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
            
            # åˆ†å‰²æ–‡æ¡£
            print("âœ‚ï¸ åˆ†å‰²æ–‡æ¡£...")
            chunks = split_documents(documents)
            print(f"âœ… æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            print("ğŸ§® åˆ›å»ºå‘é‡å­˜å‚¨...")
            self.vector_store = create_vector_store(chunks, self.vector_store_path)
            
            # æ›´æ–°å…ƒæ•°æ®
            current_docs = self._get_documents_info(documents_dir)
            self.metadata = self._load_metadata()
            self.metadata['documents'] = current_docs
            self.metadata['total_documents'] = len(documents)
            self.metadata['total_chunks'] = len(chunks)
            self._save_metadata()
            
            print("âœ… å‘é‡å­˜å‚¨é‡å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ é‡å»ºå¤±è´¥: {e}")
            return False
    
    def _incremental_update(self, documents_dir: str, new_files: List[str]) -> bool:
        """å¢é‡æ›´æ–°å‘é‡å­˜å‚¨"""
        print("ğŸ“ˆ æ‰§è¡Œå¢é‡æ›´æ–°...")
        
        try:
            # åŠ è½½ç°æœ‰å‘é‡å­˜å‚¨
            if os.path.exists(self.vector_store_path):
                print("ğŸ“‚ åŠ è½½ç°æœ‰å‘é‡å­˜å‚¨...")
                self.vector_store = load_vector_store(self.vector_store_path)
            else:
                print("ğŸ†• åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨...")
                return self._full_rebuild(documents_dir)
            
            # å¤„ç†æ–°æ–‡ä»¶
            new_documents = []
            for filename in new_files:
                file_path = os.path.join(documents_dir, filename)
                print(f"ğŸ“„ å¤„ç†æ–°æ–‡ä»¶: {filename}")
                
                # åŠ è½½å•ä¸ªæ–‡ä»¶
                if filename.endswith(('.xlsx', '.xls')):
                    from document_loader import load_excel_document
                    docs = load_excel_document(file_path)
                    new_documents.extend(docs)
                # å¯ä»¥æ·»åŠ å…¶ä»–æ–‡ä»¶ç±»å‹çš„å¤„ç†
            
            if not new_documents:
                print("âš ï¸ æ²¡æœ‰æ–°æ–‡æ¡£éœ€è¦æ·»åŠ ")
                return True
            
            print(f"âœ… æ–°å¢ {len(new_documents)} ä¸ªæ–‡æ¡£")
            
            # åˆ†å‰²æ–°æ–‡æ¡£
            new_chunks = split_documents(new_documents)
            print(f"âœ… æ–°æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(new_chunks)} ä¸ªå—")
            
            # æ·»åŠ åˆ°ç°æœ‰å‘é‡å­˜å‚¨
            print("ğŸ”— æ·»åŠ åˆ°ç°æœ‰å‘é‡å­˜å‚¨...")
            embeddings = create_embeddings()
            
            # åˆ›å»ºæ–°æ–‡æ¡£çš„å‘é‡å­˜å‚¨
            new_vector_store = FAISS.from_documents(new_chunks, embeddings)
            
            # åˆå¹¶å‘é‡å­˜å‚¨
            self.vector_store.merge_from(new_vector_store)
            
            # ä¿å­˜æ›´æ–°åçš„å‘é‡å­˜å‚¨
            self.vector_store.save_local(self.vector_store_path)
            
            # æ›´æ–°å…ƒæ•°æ®
            current_docs = self._get_documents_info(documents_dir)
            self.metadata['documents'].update({
                filename: current_docs[filename] for filename in new_files
            })
            self.metadata['total_documents'] += len(new_documents)
            self.metadata['total_chunks'] += len(new_chunks)
            self._save_metadata()
            
            print("âœ… å¢é‡æ›´æ–°å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ å¢é‡æ›´æ–°å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°å®Œå…¨é‡å»º...")
            return self._full_rebuild(documents_dir)
    
    def get_vector_store(self):
        """è·å–å‘é‡å­˜å‚¨"""
        if self.vector_store is None:
            if os.path.exists(self.vector_store_path):
                self.vector_store = load_vector_store(self.vector_store_path)
            else:
                raise ValueError("å‘é‡å­˜å‚¨ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º")
        return self.vector_store
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        self.metadata = self._load_metadata()
        return {
            'total_documents': self.metadata.get('total_documents', 0),
            'total_chunks': self.metadata.get('total_chunks', 0),
            'total_files': len(self.metadata.get('documents', {})),
            'created_time': self.metadata.get('created_time', 'æœªçŸ¥'),
            'last_updated': self.metadata.get('last_updated', 'æœªçŸ¥'),
            'version': self.metadata.get('version', '1.0')
        }


def main():
    """æµ‹è¯•å¢é‡å‘é‡å­˜å‚¨"""
    print("ğŸ§ª æµ‹è¯•å¢é‡å‘é‡å­˜å‚¨ç®¡ç†å™¨")
    print("=" * 50)
    
    manager = IncrementalVectorStore()
    
    # æ£€æŸ¥æ›´æ–°éœ€æ±‚
    update_info = manager.check_updates_needed('documents')
    print("ğŸ“Š æ›´æ–°æ£€æŸ¥ç»“æœ:")
    print(f"  éœ€è¦æ›´æ–°: {update_info['needs_update']}")
    print(f"  æ–°å¢æ–‡ä»¶: {len(update_info['new_files'])}")
    print(f"  ä¿®æ”¹æ–‡ä»¶: {len(update_info['modified_files'])}")
    print(f"  åˆ é™¤æ–‡ä»¶: {len(update_info['deleted_files'])}")
    print(f"  æœªå˜æ›´æ–‡ä»¶: {len(update_info['unchanged_files'])}")
    
    # åˆ›å»ºæˆ–æ›´æ–°å‘é‡å­˜å‚¨
    success = manager.create_or_update_vector_store('documents')
    
    if success:
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_statistics()
        print(f"\nğŸ“ˆ å‘é‡å­˜å‚¨ç»Ÿè®¡:")
        print(f"  æ–‡æ¡£æ€»æ•°: {stats['total_documents']}")
        print(f"  æ–‡æœ¬å—æ€»æ•°: {stats['total_chunks']}")
        print(f"  æ–‡ä»¶æ€»æ•°: {stats['total_files']}")
        print(f"  åˆ›å»ºæ—¶é—´: {stats['created_time']}")
        print(f"  æœ€åæ›´æ–°: {stats['last_updated']}")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
